{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ey40IrdiSghn"
      },
      "source": [
        "# CodingMindset Lab - Langchain - RAG 102"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZfDJwyRVu2SF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KMtWAT_Sgho"
      },
      "source": [
        "## Prerrequisitos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDACx_DASghp"
      },
      "source": [
        "1. Instalamos las dependencias necesarias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KY6TSO0QSghp"
      },
      "outputs": [],
      "source": [
        "#!pip install -qU langchain_experimental langchain_openai langchain_community langchain ragas faiss-cpu tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80rtKIUfSghp"
      },
      "source": [
        "2. Seteamos la API Key de OpenAI, si no lo hemos hecho previamente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1L-rVSEGSghq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaF1JyZlSghq"
      },
      "source": [
        "## Descarga de la base de conocimiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8jofDJ7Sghq"
      },
      "source": [
        "> NOTA: Podemos utilizar, cualquier documento detexto o pdf que tengamos a nuestro alcance. En caso contrario podemos usar la web the Gutenberg.org para descargar cualquier libro disponible gratuitamente\n",
        "\n",
        "- Descargamos un libro sobre el funcionamiento del cerebro en formato txt y le damos el nombre de \"the_brain\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0RsEKRFxSghq"
      },
      "outputs": [],
      "source": [
        "!wget https://gutenberg.org/cache/epub/14586/pg14586.txt -O the_brain.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2uCMTw5Sghq"
      },
      "outputs": [],
      "source": [
        "with open(\"./the_brain.txt\") as f:\n",
        "  the_brain = f.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVC5t_WMSghq"
      },
      "source": [
        "## Estrategias de Chunking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCSSKeRvSghq"
      },
      "source": [
        "### Recursive Splitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nkLuibuSghq"
      },
      "source": [
        "También conocido como \"Naive chunking\", suele ser la estrategia usada por defecto en LangChain en la mayoría de tutoriales o para el enfoque \"Naive RAG\"<br> Utiliza reglas sintácticas.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGMPSzusSghq"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=600,\n",
        "    chunk_overlap=0,\n",
        "    length_function=len,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acJLHhNfSghr"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPOa5wqUSghr"
      },
      "outputs": [],
      "source": [
        "naive_chunks = text_splitter.split_text(the_brain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7Ytb0yUSghr",
        "outputId": "62bfd5c8-8cc8-4821-aa42-bc930f900d4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "We already used the term \"external reality\" which is not defined yet. This\n",
            "fundamental term is considered as a source of information, which is not\n",
            "localized in the structure of models of the brain. I want to emphasize that\n",
            "the external reality is not a source of information, but is just considered so\n",
            "by any brain.\n",
            "\n",
            "Thus, one of the main hardware functions of the brain is to make models of the\n",
            "external reality and to predict, by simulation on the model, the possible\n",
            "evolution of the associated external reality.\n",
            "\n",
            "We already defined the reality as all the information which is or could be\n",
            "generated by a model. This means that we understand the external reality by\n",
            "the reality, which is generated by a model, which is associated with the\n",
            "external reality.\n",
            "\n",
            "Example: For a given external reality, any person makes an associated model.\n",
            "Any person has his/her own model associated to the same external reality. We\n",
            "think and act based on our own reality and not based directly on the external\n",
            "reality.\n",
            "\n",
            "In fact, external reality is rather an invention of the brain to explain its\n",
            "structure of models.\n",
            "\n",
            "THE BASIC HARDWARE ELEMENT\n",
            "\n",
            "Let's see what is the basic hardware element of a brain (human or animal).\n",
            "There are some image-type models called M-models, which are associated with\n",
            "the sense organs (eyes, ears and so on). M-models work in association with\n",
            "some YM-models, which already exist in the brain. YM-models are concept\n",
            "models. A concept-model is a simplified model which, in this way, fits a large\n",
            "class of similar models.\n",
            "\n",
            "Example of YM models: \"dog\", \"table\" and so on.\n",
            "\n",
            "M-models have to discover as many as possible entities in the external reality\n",
            "and to associate a YM model to any entity. Once an entity was firstly\n",
            "associated with a YM, M-models will predict its evolution based also on that\n",
            "YM.\n",
            "\n",
            "Example: if an entity was associated with a YM-dog, the M-model is able to\n",
            "predict how this YM performs in connection with all the other YMs of it.\n",
            "\n",
            "Any prediction of M with that YM included is compared with the information\n",
            "obtained by M from external reality. The information obtained by a M-model\n",
            "from outside during the comparison process, is called \"input reality\" (IR).\n",
            "\n",
            "We just introduced a new term as \"input reality\" or IR. IR is the information\n",
            "obtained by an M-model from outside (from external reality or from other\n",
            "models) to improve its predictions.\n",
            "\n",
            "If the prediction meets IR, then M will try another prediction to improve its\n",
            "quality. If one or more predictions do not meet IR, then M will replace that\n",
            "YR with another, and the process will continue. This process will continue so\n",
            "that all the entities which are discovered by M-models will be associated with\n",
            "some YMs and all the predictions of M must confirm the M-model, unchanged.\n",
            "Such a model is, thus, a stable model. When M is stable, all YMs are\n",
            "integrated in M in a harmonic way.\n",
            "\n",
            "The main function of M-models is to make a preliminary harmonic model (stable\n",
            "model) associated with an external reality.\n",
            "\n",
            "Conclusion: a M-model interacts with a section of the external reality. M will\n",
            "be a model made in an informational way by analogy with that section of the\n",
            "external reality. Because M is a model, all the elements are connected between\n",
            "them in a harmonic way, so that the model is stable. This stability is\n",
            "verified on and on in an automatic way, as long as a specific external reality\n",
            "is in interaction with the specific M-model.\n",
            "\n",
            "M-models interact with some other type models, called ZM-models. ZM-models\n",
            "take some information from one or more M-models and continue the construction\n",
            "of models associated with the corresponding external reality. To do this, ZM-\n",
            "models interact with the other ZM-models of the brain to improve M-models.\n",
            "\n",
            "M-models are just preliminary models based on YM-models. A ZM model will take\n",
            "any information from any other M and ZM models of the brain, to improve it.\n",
            "\n",
            "Example: an M-model is associated with a bus that transports people. A ZM-\n",
            "model takes this information and tries to see if this bus transports tourists\n",
            "or is a public transport vehicle. To do this, it will use information taken\n",
            "from any other ZM-models and M-models. The aim is to make a ZM-model, which\n",
            "reflects as well as possible a section of the external reality. Because ZM is\n",
            "a model, it is stable and because this model is integrated in a structure of\n",
            "other ZM-models, the structure of ZM-models is stable too. This problem will\n",
            "be treated later in details.\n",
            "\n",
            "ZM-models are long-range models. This term will be explained later. Here, the\n",
            "\"long-range model\" is understood as a model, which already developed its\n",
            "elements as self standing models.\n",
            "\n",
            "ZM models are the main models, which reflect the external reality.\n",
            "\n",
            "We define now two very important terms: knowledge and consciousness.\n",
            "\n",
            "Knowledge is associated with the facility to predict the evolution of the\n",
            "external reality based on a structure of harmonic/logic models. This structure\n",
            "was made by a large number of interactions with many sections of the external\n",
            "reality and so it already generated a large number of good predictions. This\n",
            "means that the only guarantee of the correctness of the knowledge is the\n",
            "confidence in that structure of models. This issue will be developed in\n",
            "details later in the book.\n",
            "\n",
            "The consciousness is the facility to make and operate a model, associated with\n",
            "the external reality, where the person itself is an element of that model.\n",
            "When such a model is activated, it will also find the position of the person\n",
            "in the model and so it will predict the position of the person in the external\n",
            "reality. This issue will also be developed in detail in another part of the\n",
            "book.\n",
            "\n",
            "We will now develop some issues associated with the term \"knowledge\". We\n",
            "already defined knowledge as the capacity to predict in a correct way the\n",
            "evolution of the external reality.\n",
            "\n",
            "Here we use the term \"correct\". Let's see what it means. This term has two\n",
            "definitions. One situation is when a model makes a prediction and the\n",
            "prediction is compared with IR. If the prediction meets IR, then the\n",
            "prediction is \"correct\". Unfortunately, there are very few situations when the\n",
            "comparison between prediction and IR is possible.\n",
            "\n",
            "For instance, building a bridge. A problem is, for instance, if the bridge\n",
            "will be stable or not in case of an earthquake. Here we need a guarantee that\n",
            "the bridge is properly built and there is no possibility to verify this based\n",
            "on IR.\n",
            "\n",
            "The second definition of the term \"correct\" is: the brain will consider as\n",
            "\"correct\" any prediction based on a harmonic/logic structure of models. To be\n",
            "harmonic, the structure was already verified, based on IR in many other\n",
            "situations. So, the only guarantee of a \"correct\" prediction is the confidence\n",
            "in that structure of models.\n",
            "\n",
            "MDT is associated with the basic hardware functions of the brain. Once we\n",
            "described the hardware structure, everything what the MDT predicts is based on\n",
            "what the hardware is able to do. What MDT says about knowledge is not another\n",
            "theory on knowledge but what the hardware is able to do.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for chunk in naive_chunks[40:55]:\n",
        "  print(chunk + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpM2VyqySghr"
      },
      "source": [
        "### Semantic Chunk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IT67pWyNSghr"
      },
      "source": [
        "El chunking semántico implica dividir el texto en segmentos cohesivos y significativos basados en el contenido semántico. Este método asegura que cada fragmento represente un tema o idea coherente, lo cual mejora la relevancia y el contexto durante los procesos de recuperación y generación de información."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eStaVzuSghr"
      },
      "source": [
        "1. Inicializamos la clase, pasándole por parámetro el embedding model que vamos a usar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GX52tvBSghr"
      },
      "outputs": [],
      "source": [
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "\n",
        "semantic_chunker = SemanticChunker(OpenAIEmbeddings(model=\"text-embedding-3-large\"), breakpoint_threshold_type=\"percentile\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_LoUk_VSghr"
      },
      "source": [
        "\n",
        "El segundo parámetro dispone de tres métodos comunes para realizar el chunking semántico, según la [documentación de LangChain](https://python.langchain.com/docs/modules/data_connection/document_transformers/semantic-chunker):\n",
        "\n",
        "- **Método del Percentil (`percentile`)**: Se calculan todas las diferencias entre frases y cualquier diferencia mayor al percentil X se utiliza para dividir.\n",
        "- **Método de la Desviación Estándar (`standard_deviation`)**: Divide en puntos donde las diferencias superen X desviaciones estándar.\n",
        "- **Método Intercuartil (`interquartile`)**: Utiliza la distancia intercuartil para determinar los puntos de división.\n",
        "\n",
        "#### Pasos Básicos:\n",
        "\n",
        "1. **División del Documento:** Fragmenta el texto en frases basándose en signos de puntuación terminativos como `.`, `?`, y `!`.\n",
        "2. **Indexación de Frases:** Asigna un índice a cada frase según su posición en el texto.\n",
        "3. **Buffering:** Añade un `buffer_size` (int) de frases alrededor de cada sentencia clave para contextualizar.\n",
        "4. **Cálculo de Distancias:** Mide las distancias semánticas entre los grupos de frases.\n",
        "5. **Unión Basada en Similitud:** Fusiona los grupos de oraciones que sean similares utilizando los límites establecidos por los métodos de chunking.\n",
        "\n",
        "> **Nota:** Este método es experimental y podría recibir mejoras y actualizaciones en el futuro próximo.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ncxmv7rOSghs"
      },
      "source": [
        "2. Creamos los chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPWkchGTSghs"
      },
      "outputs": [],
      "source": [
        "semantic_chunks = semantic_chunker.create_documents([the_brain])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frEQA--TSghs",
        "outputId": "9e509703-f80a-4f94-c66f-6bf4a1f054a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "So, the only guarantee of a \"correct\" prediction is the confidence\n",
            "in that structure of models. MDT is associated with the basic hardware functions of the brain. Once we\n",
            "described the hardware structure, everything what the MDT predicts is based on\n",
            "what the hardware is able to do. What MDT says about knowledge is not another\n",
            "theory on knowledge but what the hardware is able to do. Any experiment is based on a model.\n",
            "419\n"
          ]
        }
      ],
      "source": [
        "for semantic_chunk in semantic_chunks:\n",
        "  if \"MDT is associated with the basic\" in semantic_chunk.page_content:\n",
        "    print(semantic_chunk.page_content)\n",
        "    print(len(semantic_chunk.page_content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfNLiKweSghs"
      },
      "source": [
        "## RAG APP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWZS5YkUSghs"
      },
      "source": [
        "Vamos a crear una RAG app, usando nuestra nueva estrategia de chunking con LangChain usando LCEL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODaSQhsNSghs"
      },
      "source": [
        "### Retrieval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUQxKceASghs"
      },
      "source": [
        "En esta ocasión usaremos **FAISS (Facebook AI Similarity Search)**. la cual es una biblioteca optimizada para realizar búsquedas rápidas en grandes bases de datos de vectores almacenados en memoria, ideal para tareas de recuperación de información y sistemas de recomendación debido a su alta velocidad y escalabilidad."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFLRfEQqSghs"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "semantic_chunk_vectorstore = FAISS.from_documents(semantic_chunks, embedding=OpenAIEmbeddings(model=\"text-embedding-3-large\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odFlgpUVSghs"
      },
      "source": [
        "Limitaremos (`semantic_chunk_vectorstore`) a k = 1 para demostrar el poder de la estrategia de chunking semántico, manteniendo un conteo de tokens similar entre el contexto recuperado semánticamente y el contexto recuperado de manera simple."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJ4a3cM7Sghs"
      },
      "outputs": [],
      "source": [
        "semantic_chunk_retriever = semantic_chunk_vectorstore.as_retriever(search_kwargs={\"k\" : 1})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xs7yqPZ_Sghs",
        "outputId": "9ae868c6-06f1-46d9-80ba-532a54d2832f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(page_content=\"generated by the model). They have only descriptive definitions. Once the fundamental terms are introduced by description, all the other terms\\nhave normal definitions, which are generated by the symbolic model, by logical\\nand mathematical operations. Let's see the definitions of the terms used by the MDT theory. Model: this is a term used on large scale in science and technology. The MDT\\ntheory accepts the definition used there. A model means some fundamental elements and some fundamental relations between\\nthe elements. The elements could be of any type (physical objects, the representation of any\\nobject in any form, including pictures of any type or images of any type or\\nmathematical symbols of any type and so on). In fact, an element could be\\nassociated with anything which can be considered as an entity. The elements\\nhave some properties, which must be specified somehow. There are a number of\\nrelations between the elements, which must also be specified. An image model (or analogic model) contains an unspecified number of elements\\nand an unspecified number of relations between the elements. An image model is\\njust given as it is. It is not possible to specify in explicit and precise\\nways which are the elements and which are the relations. Examples of image models: maps, models of an object of any type, an assembly\\nof such models including any material elements (water, air and so on), any\\nrepresentation in any form of such elements. A symbolic model uses as elements letters, numbers or words.\")]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_chunk_retriever.invoke(\"what is MDT?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cTTl4QSSghs"
      },
      "source": [
        "### Augmented"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMM4lvKhSghs"
      },
      "source": [
        "- Descargamos el prompt desde ek langchain hub por comodidad, pero podriamos crear neustro propio prompt de la siguiente manera:\n",
        "````python\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "rag_template = \"\"\"\\\n",
        "Use the following context to answer the user's query. If you cannot answer, please respond with 'I don't know'.\n",
        "\n",
        "User's Query:\n",
        "{question}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(rag_template)\n",
        "\n",
        "````"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Ke__r0KSghs"
      },
      "outputs": [],
      "source": [
        "from langchain import hub\n",
        "\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P50vDNA7Sght"
      },
      "source": [
        "### Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPjy-b8gSght"
      },
      "source": [
        "- Utilizaremos `ChatOpenAI()` para mantener la simplicidad del ejemplo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FeczauLSght"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JnSfPvaSghy"
      },
      "source": [
        "### LCEL RAG Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1k0JjDJASghy"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "semantic_rag_chain = (\n",
        "    {\"context\" : semantic_chunk_retriever, \"question\" : RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQrSxnc_Sghy",
        "outputId": "ea9a1680-6dcd-4cd4-b1ab-d159e6ec1b95"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'MDT stands for Model-Driven Theory. This theory accepts the definition of a model as fundamental elements and relations between them, which could be of any type. The elements in MDT have properties that must be specified, and there are relations between the elements that must also be specified.'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_rag_chain.invoke(\"what is MDT?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tGpusa_Sghy"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}