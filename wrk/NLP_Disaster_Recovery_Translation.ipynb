{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune T5 locally for machine translation on COVID-19 Health Service Announcements with Hugging Face\n",
    "\n",
    "[![Open in SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/aws/studio-lab-examples/blob/main/natural-language-processing/NLP_Disaster_Recovery_Translation.ipynb)\n",
    "\n",
    "This notebook is designed to run within SageMaker Lab, on a `g4dn.xlarge` GPU instance. If you are not using that right now, please restart your session and select `GPU`, as this will help you train your model in a matter of tens of minutes, rather than hours.\n",
    "\n",
    "If you are ready for training a large-scale machine translation model, then please check out using Hugging Face on Amazon SageMaker! \n",
    "\n",
    "Otherwise, please enjoy this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0. Install all necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# conda environments:\n",
      "#\n",
      "base                     /ai/anaconda3\n",
      "hug                      /ai/anaconda3/envs/hug\n",
      "sagemaker-distribution     /ai/anaconda3/envs/sagemaker-distribution\n",
      "training              *  /ai/anaconda3/envs/training\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%conda env list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "\n",
    "ipywidgets\n",
    "git+https://github.com/huggingface/transformers\n",
    "datasets\n",
    "sacrebleu\n",
    "torch\n",
    "sentencepiece\n",
    "evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/transformers (from -r requirements.txt (line 3))\n",
      "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-2hsmmdbw\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-2hsmmdbw\n",
      "  Resolved https://github.com/huggingface/transformers to commit a22ff36e0e347d3d0095cccd931cbbd12b14e86a\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting ipywidgets (from -r requirements.txt (line 2))\n",
      "  Using cached ipywidgets-8.1.3-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting datasets (from -r requirements.txt (line 4))\n",
      "  Using cached datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting sacrebleu (from -r requirements.txt (line 5))\n",
      "  Downloading sacrebleu-2.4.2-py3-none-any.whl.metadata (58 kB)\n",
      "Collecting torch (from -r requirements.txt (line 6))\n",
      "  Using cached torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Collecting sentencepiece (from -r requirements.txt (line 7))\n",
      "  Using cached sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting evaluate (from -r requirements.txt (line 8))\n",
      "  Using cached evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in /ai/anaconda3/envs/training/lib/python3.10/site-packages (from ipywidgets->-r requirements.txt (line 2)) (0.2.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /ai/anaconda3/envs/training/lib/python3.10/site-packages (from ipywidgets->-r requirements.txt (line 2)) (8.25.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /ai/anaconda3/envs/training/lib/python3.10/site-packages (from ipywidgets->-r requirements.txt (line 2)) (5.14.3)\n",
      "Collecting widgetsnbextension~=4.0.11 (from ipywidgets->-r requirements.txt (line 2))\n",
      "  Using cached widgetsnbextension-4.0.11-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab-widgets~=3.0.11 (from ipywidgets->-r requirements.txt (line 2))\n",
      "  Using cached jupyterlab_widgets-3.0.11-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting filelock (from transformers==4.45.0.dev0->-r requirements.txt (line 3))\n",
      "  Using cached filelock-3.15.4-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers==4.45.0.dev0->-r requirements.txt (line 3))\n",
      "  Using cached huggingface_hub-0.24.5-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting numpy>=1.17 (from transformers==4.45.0.dev0->-r requirements.txt (line 3))\n",
      "  Using cached numpy-2.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /ai/anaconda3/envs/training/lib/python3.10/site-packages (from transformers==4.45.0.dev0->-r requirements.txt (line 3)) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /ai/anaconda3/envs/training/lib/python3.10/site-packages (from transformers==4.45.0.dev0->-r requirements.txt (line 3)) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers==4.45.0.dev0->-r requirements.txt (line 3))\n",
      "  Using cached regex-2024.7.24-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /ai/anaconda3/envs/training/lib/python3.10/site-packages (from transformers==4.45.0.dev0->-r requirements.txt (line 3)) (2.32.3)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers==4.45.0.dev0->-r requirements.txt (line 3))\n",
      "  Using cached tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers==4.45.0.dev0->-r requirements.txt (line 3))\n",
      "  Downloading safetensors-0.4.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tqdm>=4.27 (from transformers==4.45.0.dev0->-r requirements.txt (line 3))\n",
      "  Using cached tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting pyarrow>=15.0.0 (from datasets->-r requirements.txt (line 4))\n",
      "  Using cached pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting pyarrow-hotfix (from datasets->-r requirements.txt (line 4))\n",
      "  Using cached pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets->-r requirements.txt (line 4))\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets->-r requirements.txt (line 4))\n",
      "  Using cached pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Collecting xxhash (from datasets->-r requirements.txt (line 4))\n",
      "  Using cached xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets->-r requirements.txt (line 4))\n",
      "  Using cached multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.5.0,>=2023.1.0 (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets->-r requirements.txt (line 4))\n",
      "  Using cached fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets->-r requirements.txt (line 4))\n",
      "  Downloading aiohttp-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
      "Collecting portalocker (from sacrebleu->-r requirements.txt (line 5))\n",
      "  Using cached portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting tabulate>=0.8.9 (from sacrebleu->-r requirements.txt (line 5))\n",
      "  Using cached tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Collecting colorama (from sacrebleu->-r requirements.txt (line 5))\n",
      "  Using cached colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Collecting lxml (from sacrebleu->-r requirements.txt (line 5))\n",
      "  Downloading lxml-5.3.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /ai/anaconda3/envs/training/lib/python3.10/site-packages (from torch->-r requirements.txt (line 6)) (4.11.0)\n",
      "Collecting sympy (from torch->-r requirements.txt (line 6))\n",
      "  Downloading sympy-1.13.2-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch->-r requirements.txt (line 6))\n",
      "  Using cached networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: jinja2 in /ai/anaconda3/envs/training/lib/python3.10/site-packages (from torch->-r requirements.txt (line 6)) (3.1.4)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->-r requirements.txt (line 6))\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->-r requirements.txt (line 6))\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->-r requirements.txt (line 6))\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->-r requirements.txt (line 6))\n",
      "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->-r requirements.txt (line 6))\n",
      "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->-r requirements.txt (line 6))\n",
      "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->-r requirements.txt (line 6))\n",
      "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->-r requirements.txt (line 6))\n",
      "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->-r requirements.txt (line 6))\n",
      "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch->-r requirements.txt (line 6))\n",
      "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->-r requirements.txt (line 6))\n",
      "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.0.0 (from torch->-r requirements.txt (line 6))\n",
      "  Using cached triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->-r requirements.txt (line 6))\n",
      "  Downloading nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets->-r requirements.txt (line 4))\n",
      "  Downloading aiohappyeyeballs-2.3.5-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets->-r requirements.txt (line 4))\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /ai/anaconda3/envs/training/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 4)) (23.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets->-r requirements.txt (line 4))\n",
      "  Using cached frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets->-r requirements.txt (line 4))\n",
      "  Using cached multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets->-r requirements.txt (line 4))\n",
      "  Using cached yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets->-r requirements.txt (line 4))\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: decorator in /ai/anaconda3/envs/training/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 2)) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /ai/anaconda3/envs/training/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 2)) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /ai/anaconda3/envs/training/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 2)) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /ai/anaconda3/envs/training/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 2)) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /ai/anaconda3/envs/training/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 2)) (2.15.1)\n",
      "Requirement already satisfied: stack-data in /ai/anaconda3/envs/training/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 2)) (0.2.0)\n",
      "Requirement already satisfied: exceptiongroup in /ai/anaconda3/envs/training/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 2)) (1.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /ai/anaconda3/envs/training/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 2)) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /ai/anaconda3/envs/training/lib/python3.10/site-packages (from requests->transformers==4.45.0.dev0->-r requirements.txt (line 3)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /ai/anaconda3/envs/training/lib/python3.10/site-packages (from requests->transformers==4.45.0.dev0->-r requirements.txt (line 3)) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /ai/anaconda3/envs/training/lib/python3.10/site-packages (from requests->transformers==4.45.0.dev0->-r requirements.txt (line 3)) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /ai/anaconda3/envs/training/lib/python3.10/site-packages (from requests->transformers==4.45.0.dev0->-r requirements.txt (line 3)) (2024.7.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /ai/anaconda3/envs/training/lib/python3.10/site-packages (from jinja2->torch->-r requirements.txt (line 6)) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /ai/anaconda3/envs/training/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 4)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /ai/anaconda3/envs/training/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 4)) (2024.1)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets->-r requirements.txt (line 4))\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch->-r requirements.txt (line 6))\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /ai/anaconda3/envs/training/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets->-r requirements.txt (line 2)) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /ai/anaconda3/envs/training/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets->-r requirements.txt (line 2)) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /ai/anaconda3/envs/training/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets->-r requirements.txt (line 2)) (0.2.5)\n",
      "Requirement already satisfied: six>=1.5 in /ai/anaconda3/envs/training/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->-r requirements.txt (line 4)) (1.16.0)\n",
      "Requirement already satisfied: executing in /ai/anaconda3/envs/training/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets->-r requirements.txt (line 2)) (0.8.3)\n",
      "Requirement already satisfied: asttokens in /ai/anaconda3/envs/training/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets->-r requirements.txt (line 2)) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in /ai/anaconda3/envs/training/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets->-r requirements.txt (line 2)) (0.2.2)\n",
      "Using cached ipywidgets-8.1.3-py3-none-any.whl (139 kB)\n",
      "Using cached datasets-2.20.0-py3-none-any.whl (547 kB)\n",
      "Downloading sacrebleu-2.4.2-py3-none-any.whl (106 kB)\n",
      "Using cached torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl (797.2 MB)\n",
      "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Using cached triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
      "Using cached sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Using cached evaluate-0.4.2-py3-none-any.whl (84 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
      "Downloading aiohttp-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached huggingface_hub-0.24.5-py3-none-any.whl (417 kB)\n",
      "Using cached jupyterlab_widgets-3.0.11-py3-none-any.whl (214 kB)\n",
      "Using cached numpy-2.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.5 MB)\n",
      "Using cached pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
      "Using cached regex-2024.7.24-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (776 kB)\n",
      "Downloading safetensors-0.4.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Using cached tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "Using cached tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Using cached widgetsnbextension-4.0.11-py3-none-any.whl (2.3 MB)\n",
      "Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Using cached filelock-3.15.4-py3-none-any.whl (16 kB)\n",
      "Downloading lxml-5.3.0-cp310-cp310-manylinux_2_28_x86_64.whl (5.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m01\u001b[0m\n",
      "\u001b[?25hUsing cached multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "Using cached networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "Using cached pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
      "Using cached portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
      "Using cached pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading sympy-1.13.2-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Downloading aiohappyeyeballs-2.3.5-py3-none-any.whl (12 kB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Using cached frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Using cached yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "Downloading nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: transformers\n",
      "  Building wheel for transformers (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-4.45.0.dev0-py3-none-any.whl size=9527412 sha256=2721664c41c958ca5a094ea9e5cfd01d266d5fbc169c7abbc4a6c785dd3fc9b5\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-vw6w7vff/wheels/c0/14/d6/6c9a5582d2ac191ec0a483be151a4495fe1eb2a6706ca49f1b\n",
      "Successfully built transformers\n",
      "Installing collected packages: sentencepiece, mpmath, xxhash, widgetsnbextension, tzdata, tqdm, tabulate, sympy, safetensors, regex, pyarrow-hotfix, portalocker, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, multidict, lxml, jupyterlab-widgets, fsspec, frozenlist, filelock, dill, colorama, async-timeout, aiohappyeyeballs, yarl, triton, sacrebleu, pyarrow, pandas, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, huggingface-hub, aiosignal, tokenizers, nvidia-cusolver-cu12, aiohttp, transformers, torch, ipywidgets, datasets, evaluate\n",
      "Successfully installed aiohappyeyeballs-2.3.5 aiohttp-3.10.3 aiosignal-1.3.1 async-timeout-4.0.3 colorama-0.4.6 datasets-2.20.0 dill-0.3.8 evaluate-0.4.2 filelock-3.15.4 frozenlist-1.4.1 fsspec-2024.5.0 huggingface-hub-0.24.5 ipywidgets-8.1.3 jupyterlab-widgets-3.0.11 lxml-5.3.0 mpmath-1.3.0 multidict-6.0.5 multiprocess-0.70.16 networkx-3.3 numpy-2.0.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 pandas-2.2.2 portalocker-2.10.1 pyarrow-17.0.0 pyarrow-hotfix-0.6 regex-2024.7.24 sacrebleu-2.4.2 safetensors-0.4.4 sentencepiece-0.2.0 sympy-1.13.2 tabulate-0.9.0 tokenizers-0.19.1 torch-2.4.0 tqdm-4.66.5 transformers-4.45.0.dev0 triton-3.0.0 tzdata-2024.1 widgetsnbextension-4.0.11 xxhash-3.4.1 yarl-1.9.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "# make sure to restart your kernel to use the newly install packages\n",
    "# IPython.Application.instance().kernel.do_shutdown(True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Explore the available datasets on Translators without Borders \n",
    "Then, download a pair you would like to use for training a language translation model. The steps below download the translation pairs for English to Spanish, but you are welcome to modify these and use a different pair if you prefer.\n",
    "\n",
    "Overall site page: https://tico-19.github.io/\n",
    "\n",
    "Page with all language pairs: https://tico-19.github.io/memories.html \n",
    "\n",
    "Scroll through all supported language pairs and pick your favorite. We'll demonstrate English to Spanish, `en-to-es`\n",
    "\n",
    "Copy the link to that pair, for `en-to-es` it looks like this:\n",
    "- https://tico-19.github.io/data/TM/all.en-es-LA.tmx.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_my_data = 'https://tico-19.github.io/data/TM/all.en-es-LA.tmx.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-08-13 21:40:58--  https://tico-19.github.io/data/TM/all.en-es-LA.tmx.zip\n",
      "Resolving tico-19.github.io (tico-19.github.io)... 185.199.108.153, 185.199.111.153, 185.199.110.153, ...\n",
      "Connecting to tico-19.github.io (tico-19.github.io)|185.199.108.153|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 381511 (373K) [application/zip]\n",
      "Saving to: ‘all.en-es-LA.tmx.zip’\n",
      "\n",
      "all.en-es-LA.tmx.zi 100%[===================>] 372,57K  --.-KB/s    in 0,04s   \n",
      "\n",
      "2024-08-13 21:40:59 (8,81 MB/s) - ‘all.en-es-LA.tmx.zip’ saved [381511/381511]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget {path_to_my_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all.en-es-LA.tmx.zip\n",
      "all.en-es-LA.tmx\n"
     ]
    }
   ],
   "source": [
    "local_file = path_to_my_data.split('/')[-1]\n",
    "print (local_file)\n",
    "filename = local_file.split('.zip')[0]\n",
    "print (filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  all.en-es-LA.tmx.zip\n",
      "  inflating: all.en-es-LA.tmx        \n"
     ]
    }
   ],
   "source": [
    "!unzip {local_file}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Extract data from `.tmx` file type \n",
    "Next, you can use this local function to extract data from the `.tmx` file type and format for local training with Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paste the name of your file and language codes here\n",
    "source_code_1 = 'en'\n",
    "target_code_2 =  'es'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tmx(filename, source_code_1, target_code_2):\n",
    "    '''\n",
    "    Takes a local TMX filename and codes for source and target languages. \n",
    "    Walks through your file, row by row, looking for tmx / html specific formatting.\n",
    "    If there's a regex match, will clean your string and add to a dictionary for downstream pandas formatting.\n",
    "    '''\n",
    "    \n",
    "    data = {source_code_1:[], target_code_2:[]}\n",
    "\n",
    "    with open(filename) as f:\n",
    "\n",
    "        for row in f.readlines():\n",
    "\n",
    "            if not row.endswith('</seg></tuv>\\n'):\n",
    "                continue\n",
    "\n",
    "            if row.startswith('<seg>'):\n",
    "\n",
    "                st_1 = row.strip()\n",
    "\n",
    "                st_1 = st_1.replace('<seg>', '')\n",
    "                st_1 = st_1.replace('</seg></tuv>', '')\n",
    "\n",
    "                data[source_code_1].append(st_1)\n",
    "\n",
    "            # when you use your own target code, remove the -LA string \n",
    "            if row.startswith('<tuv xml:lang=\"{}-LA\"><seg>'.format(target_code_2)):\n",
    "\n",
    "                st_2 = row.strip()\n",
    "                # when you use your own target code, remove the -LA string \n",
    "                st_2 = st_2.replace('<tuv xml:lang=\"{}-LA\"><seg>'.format(target_code_2), '')\n",
    "                st_2 = st_2.replace('</seg></tuv>', '')\n",
    "\n",
    "                data[target_code_2].append(st_2)\n",
    "                \n",
    "        return data\n",
    "\n",
    "data = parse_tmx(filename, source_code_1, target_code_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this makes sure you got actual pairs\n",
    "assert len(data[source_code_1]) == len(data[target_code_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>es</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>about how long have these symptoms been going on?</td>\n",
       "      <td>¿cuánto hace más o menos que tiene estos sínto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>and all chest pain should be treated this way ...</td>\n",
       "      <td>y siempre el dolor de pecho debe tratarse de e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>and along with a fever</td>\n",
       "      <td>y también fiebre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>and also needs to be checked your cholesterol ...</td>\n",
       "      <td>y también debe controlarse su colesterol y pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>and are you having a fever now?</td>\n",
       "      <td>¿y tiene fiebre ahora?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  en  \\\n",
       "0  about how long have these symptoms been going on?   \n",
       "1  and all chest pain should be treated this way ...   \n",
       "2                             and along with a fever   \n",
       "3  and also needs to be checked your cholesterol ...   \n",
       "4                    and are you having a fever now?   \n",
       "\n",
       "                                                  es  \n",
       "0  ¿cuánto hace más o menos que tiene estos sínto...  \n",
       "1  y siempre el dolor de pecho debe tratarse de e...  \n",
       "2                                   y también fiebre  \n",
       "3  y también debe controlarse su colesterol y pre...  \n",
       "4                             ¿y tiene fiebre ahora?  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame.from_dict(data, orient = 'columns')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to disk in case you need to restart your kernel later\n",
    "df.to_csv('language_pairs.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Format extracted data for machine translation with Hugging Face\n",
    "Core examples available right here: https://github.com/huggingface/transformers/tree/master/examples/pytorch/translation \n",
    "\n",
    "Guidance on formatting for Hugging Face datasets here:\n",
    "https://huggingface.co/docs/datasets/loading_datasets.html#json-files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>es</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>about how long have these symptoms been going on?</td>\n",
       "      <td>¿cuánto hace más o menos que tiene estos sínto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>and all chest pain should be treated this way ...</td>\n",
       "      <td>y siempre el dolor de pecho debe tratarse de e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>and along with a fever</td>\n",
       "      <td>y también fiebre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>and also needs to be checked your cholesterol ...</td>\n",
       "      <td>y también debe controlarse su colesterol y pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>and are you having a fever now?</td>\n",
       "      <td>¿y tiene fiebre ahora?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  en  \\\n",
       "0  about how long have these symptoms been going on?   \n",
       "1  and all chest pain should be treated this way ...   \n",
       "2                             and along with a fever   \n",
       "3  and also needs to be checked your cholesterol ...   \n",
       "4                    and are you having a fever now?   \n",
       "\n",
       "                                                  es  \n",
       "0  ¿cuánto hace más o menos que tiene estos sínto...  \n",
       "1  y siempre el dolor de pecho debe tratarse de e...  \n",
       "2                                   y también fiebre  \n",
       "3  y también debe controlarse su colesterol y pre...  \n",
       "4                             ¿y tiene fiebre ahora?  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('language_pairs.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task of translation supports only custom JSONLINES files, with each line being a dictionary with a key \"translation\" and its value another dictionary whose keys is the language pair. For example:\n",
    "\n",
    "`{ \"translation\": { \"en\": \"Others have dismissed him as a joke.\", \"ro\": \"Alții l-au numit o glumă.\" } }\n",
    "{ \"translation\": { \"en\": \"And some are holding out for an implosion.\", \"ro\": \"Iar alții așteaptă implozia.\" } }`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "objs = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    \n",
    "    obj = {\"translation\": {source_code_1: row[source_code_1], target_code_2: row[target_code_2]}} \n",
    "    objs.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation': {'en': 'about how long have these symptoms been going on?',\n",
       "   'es': '¿cuánto hace más o menos que tiene estos síntomas?'}},\n",
       " {'translation': {'en': 'and all chest pain should be treated this way especially with your age',\n",
       "   'es': 'y siempre el dolor de pecho debe tratarse de esta manera, en especial a su edad'}},\n",
       " {'translation': {'en': 'and along with a fever', 'es': 'y también fiebre'}},\n",
       " {'translation': {'en': 'and also needs to be checked your cholesterol blood pressure',\n",
       "   'es': 'y también debe controlarse su colesterol y presión arterial'}},\n",
       " {'translation': {'en': 'and are you having a fever now?',\n",
       "   'es': '¿y tiene fiebre ahora?'}}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "!mkdir data\n",
    "with open('data/train.json', 'w') as f:\n",
    "    for row in objs:\n",
    "        j = json.dumps(row, ensure_ascii = False)\n",
    "        f.write(j)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 - Finetune a machine translation model locally\n",
    "Do to this, let's first download the raw Python file we need from Hugging Face to finetune our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-08-13 21:43:49--  https://raw.githubusercontent.com/huggingface/transformers/master/examples/pytorch/translation/run_translation.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 30439 (30K) [text/plain]\n",
      "Saving to: ‘run_translation.py’\n",
      "\n",
      "run_translation.py  100%[===================>]  29,73K  --.-KB/s    in 0,005s  \n",
      "\n",
      "2024-08-13 21:43:49 (6,37 MB/s) - ‘run_translation.py’ saved [30439/30439]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/pytorch/translation/run_translation.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error message indicates that you need to install or update the `accelerate` package to version 0.21.0 or higher to use the `Trainer` with PyTorch. The error occurs when the script tries to set up the device for training but finds that the required version of `accelerate` is not installed.\n",
    "\n",
    "To resolve this issue, you can install the required package by running the following command:\n",
    "\n",
    "```bash\n",
    "pip install 'accelerate>=0.21.0'\n",
    "```\n",
    "\n",
    "Alternatively, you can install the required dependencies for using `transformers` with PyTorch by running:\n",
    "\n",
    "```bash\n",
    "pip install transformers[torch]\n",
    "```\n",
    "\n",
    "This should resolve the `ImportError`, and you should be able to run your translation training script successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install 'accelerate>=0.21.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/13/2024 22:56:49 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "08/13/2024 22:56:49 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=None,\n",
      "eval_strategy=no,\n",
      "eval_use_gather_object=False,\n",
      "evaluation_strategy=None,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_config=None,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=output/tst-translation/runs/Aug13_22-56-49_asdaqewg,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=output/tst-translation,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=4,\n",
      "predict_with_generate=True,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=output/tst-translation,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=epoch,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "Using custom data configuration default-d80099dafcc33fdb\n",
      "08/13/2024 22:56:49 - INFO - datasets.builder - Using custom data configuration default-d80099dafcc33fdb\n",
      "Loading Dataset Infos from /ai/anaconda3/envs/training/lib/python3.10/site-packages/datasets/packaged_modules/json\n",
      "08/13/2024 22:56:49 - INFO - datasets.info - Loading Dataset Infos from /ai/anaconda3/envs/training/lib/python3.10/site-packages/datasets/packaged_modules/json\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "08/13/2024 22:56:49 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from /home/nuevo/.cache/huggingface/datasets/json/default-d80099dafcc33fdb/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a\n",
      "08/13/2024 22:56:49 - INFO - datasets.info - Loading Dataset info from /home/nuevo/.cache/huggingface/datasets/json/default-d80099dafcc33fdb/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a\n",
      "Found cached dataset json (/home/nuevo/.cache/huggingface/datasets/json/default-d80099dafcc33fdb/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a)\n",
      "08/13/2024 22:56:50 - INFO - datasets.builder - Found cached dataset json (/home/nuevo/.cache/huggingface/datasets/json/default-d80099dafcc33fdb/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a)\n",
      "Loading Dataset info from /home/nuevo/.cache/huggingface/datasets/json/default-d80099dafcc33fdb/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a\n",
      "08/13/2024 22:56:50 - INFO - datasets.info - Loading Dataset info from /home/nuevo/.cache/huggingface/datasets/json/default-d80099dafcc33fdb/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a\n",
      "[INFO|configuration_utils.py:733] 2024-08-13 22:56:50,146 >> loading configuration file config.json from cache at /home/nuevo/.cache/huggingface/hub/models--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-08-13 22:56:50,150 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-small\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.45.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-08-13 22:56:50,282 >> loading file spiece.model from cache at /home/nuevo/.cache/huggingface/hub/models--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-08-13 22:56:50,282 >> loading file tokenizer.json from cache at /home/nuevo/.cache/huggingface/hub/models--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-08-13 22:56:50,283 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-08-13 22:56:50,283 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-08-13 22:56:50,283 >> loading file tokenizer_config.json from cache at /home/nuevo/.cache/huggingface/hub/models--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/tokenizer_config.json\n",
      "[INFO|modeling_utils.py:3657] 2024-08-13 22:56:50,455 >> loading weights file model.safetensors from cache at /home/nuevo/.cache/huggingface/hub/models--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/model.safetensors\n",
      "[INFO|configuration_utils.py:1038] 2024-08-13 22:56:50,460 >> Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4489] 2024-08-13 22:56:50,606 >> All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:4497] 2024-08-13 22:56:50,606 >> All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "[INFO|configuration_utils.py:993] 2024-08-13 22:56:50,745 >> loading configuration file generation_config.json from cache at /home/nuevo/.cache/huggingface/hub/models--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/generation_config.json\n",
      "[INFO|configuration_utils.py:1038] 2024-08-13 22:56:50,745 >> Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Loading cached processed dataset at /home/nuevo/.cache/huggingface/datasets/json/default-d80099dafcc33fdb/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a/cache-0f10fdd348d7db67.arrow\n",
      "08/13/2024 22:56:50 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/nuevo/.cache/huggingface/datasets/json/default-d80099dafcc33fdb/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a/cache-0f10fdd348d7db67.arrow\n",
      "[INFO|trainer.py:2160] 2024-08-13 22:56:52,659 >> ***** Running training *****\n",
      "[INFO|trainer.py:2161] 2024-08-13 22:56:52,659 >>   Num examples = 3,071\n",
      "[INFO|trainer.py:2162] 2024-08-13 22:56:52,659 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:2163] 2024-08-13 22:56:52,659 >>   Instantaneous batch size per device = 4\n",
      "[INFO|trainer.py:2166] 2024-08-13 22:56:52,659 >>   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "[INFO|trainer.py:2167] 2024-08-13 22:56:52,659 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:2168] 2024-08-13 22:56:52,660 >>   Total optimization steps = 2,304\n",
      "[INFO|trainer.py:2169] 2024-08-13 22:56:52,660 >>   Number of trainable parameters = 60,506,624\n",
      "{'loss': 2.6045, 'grad_norm': 3.119659900665283, 'learning_rate': 3.914930555555556e-05, 'epoch': 0.65}\n",
      " 33%|█████████████▎                          | 768/2304 [02:00<03:32,  7.25it/s][INFO|trainer.py:3548] 2024-08-13 22:58:52,860 >> Saving model checkpoint to output/tst-translation/checkpoint-768\n",
      "[INFO|configuration_utils.py:472] 2024-08-13 22:58:52,861 >> Configuration saved in output/tst-translation/checkpoint-768/config.json\n",
      "[INFO|configuration_utils.py:807] 2024-08-13 22:58:52,861 >> Configuration saved in output/tst-translation/checkpoint-768/generation_config.json\n",
      "[INFO|modeling_utils.py:2778] 2024-08-13 22:58:55,583 >> Model weights saved in output/tst-translation/checkpoint-768/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-08-13 22:58:55,585 >> tokenizer config file saved in output/tst-translation/checkpoint-768/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-08-13 22:58:55,586 >> Special tokens file saved in output/tst-translation/checkpoint-768/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:175] 2024-08-13 22:58:55,588 >> Copy vocab file to output/tst-translation/checkpoint-768/spiece.model\n",
      "{'loss': 2.274, 'grad_norm': 3.032119035720825, 'learning_rate': 2.8298611111111113e-05, 'epoch': 1.3}\n",
      "{'loss': 2.1279, 'grad_norm': 2.6983375549316406, 'learning_rate': 1.7447916666666666e-05, 'epoch': 1.95}\n",
      " 67%|██████████████████████████             | 1536/2304 [04:08<01:54,  6.70it/s][INFO|trainer.py:3548] 2024-08-13 23:01:01,494 >> Saving model checkpoint to output/tst-translation/checkpoint-1536\n",
      "[INFO|configuration_utils.py:472] 2024-08-13 23:01:01,496 >> Configuration saved in output/tst-translation/checkpoint-1536/config.json\n",
      "[INFO|configuration_utils.py:807] 2024-08-13 23:01:01,496 >> Configuration saved in output/tst-translation/checkpoint-1536/generation_config.json\n",
      "[INFO|modeling_utils.py:2778] 2024-08-13 23:01:04,205 >> Model weights saved in output/tst-translation/checkpoint-1536/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-08-13 23:01:04,208 >> tokenizer config file saved in output/tst-translation/checkpoint-1536/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-08-13 23:01:04,208 >> Special tokens file saved in output/tst-translation/checkpoint-1536/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:175] 2024-08-13 23:01:04,295 >> Copy vocab file to output/tst-translation/checkpoint-1536/spiece.model\n",
      "{'loss': 2.0539, 'grad_norm': 2.5380167961120605, 'learning_rate': 6.597222222222223e-06, 'epoch': 2.6}\n",
      "100%|███████████████████████████████████████| 2304/2304 [06:21<00:00,  6.42it/s][INFO|trainer.py:3548] 2024-08-13 23:03:13,696 >> Saving model checkpoint to output/tst-translation/checkpoint-2304\n",
      "[INFO|configuration_utils.py:472] 2024-08-13 23:03:13,697 >> Configuration saved in output/tst-translation/checkpoint-2304/config.json\n",
      "[INFO|configuration_utils.py:807] 2024-08-13 23:03:13,698 >> Configuration saved in output/tst-translation/checkpoint-2304/generation_config.json\n",
      "[INFO|modeling_utils.py:2778] 2024-08-13 23:03:16,301 >> Model weights saved in output/tst-translation/checkpoint-2304/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-08-13 23:03:16,304 >> tokenizer config file saved in output/tst-translation/checkpoint-2304/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-08-13 23:03:16,304 >> Special tokens file saved in output/tst-translation/checkpoint-2304/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:175] 2024-08-13 23:03:16,307 >> Copy vocab file to output/tst-translation/checkpoint-2304/spiece.model\n",
      "[INFO|trainer.py:3548] 2024-08-13 23:03:21,557 >> Saving model checkpoint to output/tst-translation/checkpoint-2304\n",
      "[INFO|configuration_utils.py:472] 2024-08-13 23:03:21,558 >> Configuration saved in output/tst-translation/checkpoint-2304/config.json\n",
      "[INFO|configuration_utils.py:807] 2024-08-13 23:03:21,559 >> Configuration saved in output/tst-translation/checkpoint-2304/generation_config.json\n",
      "[INFO|modeling_utils.py:2778] 2024-08-13 23:03:24,268 >> Model weights saved in output/tst-translation/checkpoint-2304/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-08-13 23:03:24,271 >> tokenizer config file saved in output/tst-translation/checkpoint-2304/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-08-13 23:03:24,271 >> Special tokens file saved in output/tst-translation/checkpoint-2304/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:175] 2024-08-13 23:03:24,273 >> Copy vocab file to output/tst-translation/checkpoint-2304/spiece.model\n",
      "[INFO|trainer.py:2420] 2024-08-13 23:03:29,642 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 396.982, 'train_samples_per_second': 23.208, 'train_steps_per_second': 5.804, 'train_loss': 2.2342363463507757, 'epoch': 3.0}\n",
      "100%|███████████████████████████████████████| 2304/2304 [06:36<00:00,  5.80it/s]\n",
      "[INFO|trainer.py:3548] 2024-08-13 23:03:29,644 >> Saving model checkpoint to output/tst-translation\n",
      "[INFO|configuration_utils.py:472] 2024-08-13 23:03:29,646 >> Configuration saved in output/tst-translation/config.json\n",
      "[INFO|configuration_utils.py:807] 2024-08-13 23:03:29,646 >> Configuration saved in output/tst-translation/generation_config.json\n",
      "[INFO|modeling_utils.py:2778] 2024-08-13 23:03:32,251 >> Model weights saved in output/tst-translation/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-08-13 23:03:32,254 >> tokenizer config file saved in output/tst-translation/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-08-13 23:03:32,254 >> Special tokens file saved in output/tst-translation/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:175] 2024-08-13 23:03:32,256 >> Copy vocab file to output/tst-translation/spiece.model\n",
      "***** train metrics *****\n",
      "  epoch                    =        3.0\n",
      "  total_flos               =   138239GF\n",
      "  train_loss               =     2.2342\n",
      "  train_runtime            = 0:06:36.98\n",
      "  train_samples            =       3071\n",
      "  train_samples_per_second =     23.208\n",
      "  train_steps_per_second   =      5.804\n",
      "[INFO|modelcard.py:449] 2024-08-13 23:03:32,731 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Translation', 'type': 'translation'}}\n"
     ]
    }
   ],
   "source": [
    "# full hugging face Trainer API args available here\n",
    "# https://github.com/huggingface/transformers/blob/de635af3f1ef740aa32f53a91473269c6435e19e/src/transformers/training_args.py\n",
    "# T5 trainig args available here\n",
    "# https://huggingface.co/transformers/model_doc/t5.html#t5config\n",
    "!python run_translation.py \\\n",
    "    --model_name_or_path t5-small \\\n",
    "    --do_train \\\n",
    "    --source_lang en \\\n",
    "    --target_lang es \\\n",
    "    --source_prefix \"translate English to Spanish: \" \\\n",
    "    --train_file data/train.json \\\n",
    "    --output_dir output/tst-translation \\\n",
    "    --per_device_train_batch_size=4 \\\n",
    "    --per_device_eval_batch_size=4 \\\n",
    "    --overwrite_output_dir \\\n",
    "    --predict_with_generate \\\n",
    "    --save_strategy epoch \\\n",
    "    --num_train_epochs 3\n",
    "#     --do_eval \\\n",
    "#     --validation_file path_to_jsonlines_file \\\n",
    "#     --dataset_name cov-19 \\\n",
    "#     --dataset_config_name en-es \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_results.json  generation_config.json   tokenizer_config.json\n",
      "checkpoint-1536   model.safetensors\t   tokenizer.json\n",
      "checkpoint-2304   README.md\t\t   trainer_state.json\n",
      "checkpoint-768\t  special_tokens_map.json  training_args.bin\n",
      "config.json\t  spiece.model\t\t   train_results.json\n"
     ]
    }
   ],
   "source": [
    "!ls output/tst-translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5. Test your newly fine-tuned translation model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The warning message you're seeing indicates that the `AutoModelWithLMHead` class is deprecated and will be removed in a future version of the `transformers` library. Instead, you should use the more specific classes depending on the type of model you are working with. Since you are using a T5 model, which is an encoder-decoder model, you should use `AutoModelForSeq2SeqLM`.\n",
    "\n",
    "Here’s how you can update your code to avoid the deprecation warning:\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "# Load the model from the trained output directory\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('output/tst-translation')\n",
    "```\n",
    "\n",
    "This should eliminate the warning and ensure your code is up-to-date with the latest practices in the `transformers` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import AutoTokenizer, AutoModelWithLMHead \n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "#model = AutoModelWithLMHead.from_pretrained(pretrained_model_name_or_path = 'output/tst-translation')\n",
    "\n",
    "#Here’s how you can update your code to avoid the deprecation warning:\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "# Load the model from the trained output directory\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('output/tst-translation')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# line to make sure your model supports local inference\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's test it! Remember that, in using the default settings of only 3 epoch, your translation is probably not going to be SOTA. For achieving state of the art, (SOTA), we recommend migrating to Amazon SageMaker to scale up and out. Scaling up means moving your code to a more advanced compute type, such as a p4 series or even Trainium. Scaling out means adding more compute, so going from 1 to many instances. Using the entire AWS cloud you can train for much longer periods of time on much larger datasets, which can directly translate to a more accurate model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the translation outputs are not as accurate as expected. Several phrases are incorrect or incomplete, which could be due to a few reasons, such as:\n",
    "\n",
    "1. **Model Quality**: The T5-small model is a small variant of the T5 family and may not provide highly accurate translations for all inputs, especially when trained on a small dataset.\n",
    "\n",
    "2. **Training Dataset**: If the training dataset (`data/train.json`) is small or not well-aligned with the task, the model may not generalize well to unseen data.\n",
    "\n",
    "3. **Generation Settings**: The warning about `max_length` suggests that the default maximum length for generated sequences might be too short, truncating some translations. Setting a higher value for `max_new_tokens` could yield more complete translations.\n",
    "\n",
    "To improve the results, you can try the following:\n",
    "\n",
    "### 1. Adjust the `max_new_tokens`\n",
    "Increase the `max_new_tokens` to allow the model to generate longer sequences:\n",
    "\n",
    "```python\n",
    "outputs = model.generate(input_ids, max_new_tokens=50)  # Adjust this value as needed\n",
    "```\n",
    "\n",
    "### 2. Fine-tune the Model Further\n",
    "If possible, fine-tune the model with more relevant data to improve its performance on your specific task.\n",
    "\n",
    "### 3. Use a Larger Model\n",
    "Consider using a larger variant of T5, such as `t5-base` or `t5-large`, which may provide better translations.\n",
    "\n",
    "### 4. Post-Processing\n",
    "Post-process the output to fix common errors, like missing or incorrect words. This could include rules or another model to refine the translations.\n",
    "\n",
    "If you apply these suggestions, you should see improvements in the translation quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "about how long have these symptoms been going on? en el trabajo de los sntomas?\n",
      "and all chest pain should be treated this way especially with your age\t y todos los dolores de la población del trabajo es tratar el felo, en specialmente en el âge\n",
      "and along with a fever\t y a las fiebas\n",
      "and also needs to be checked your cholesterol blood pressure y es necesario a verificar la presión arterial del cholesterol\n",
      "and are you having a fever now?\t y tiene una fiere?\n",
      "and are you having any of the following symptoms with your chest pain y tiene el sntomas más más en el dolor de la población\n",
      "and are you having a runny nose? y tiene un nase agua?\n",
      "and are you having this chest pain now? y tiene el dolor de la población?\n",
      "and besides do you have difficulty breathing y ahora ahora a las dificultas respirar\n",
      "and can you tell me what other symptoms are you having along with this? y tu pueden me dire quels sntomas tienes en el trabajo?\n",
      "and does this pain move from your chest? y se movió el dolor de ta pobla?\n",
      "and drink lots of fluids y tudo a lot de fluides\n",
      "and how high has your fever been y el más ao de la fièvre\n",
      "and i have a cough too y es un toso\n",
      "and i have a little cold and a cough y es un ao recio y un tos\n",
      "and i'm really having some bad chest pain today y es el trabajo de las dolores de las poblas en el momento\n"
     ]
    }
   ],
   "source": [
    "input_sequences = ['about how long have these symptoms been going on?',\t\n",
    "'and all chest pain should be treated this way especially with your age\t',\n",
    "'and along with a fever\t',\n",
    "'and also needs to be checked your cholesterol blood pressure',\t\n",
    "'and are you having a fever now?\t',\n",
    "'and are you having any of the following symptoms with your chest pain',\t\n",
    "'and are you having a runny nose?',\t\n",
    "'and are you having this chest pain now?',\n",
    "'and besides do you have difficulty breathing',\n",
    "'and can you tell me what other symptoms are you having along with this?',\n",
    "'and does this pain move from your chest?',\n",
    "'and drink lots of fluids',\n",
    "'and how high has your fever been',\n",
    "'and i have a cough too',\n",
    "'and i have a little cold and a cough',\n",
    "'''and i'm really having some bad chest pain today''']\n",
    "\n",
    "task_prefix = \"translate English to Spanish: \"\n",
    "\n",
    "for i in input_sequences:\n",
    "    input_ids = tokenizer('''{} {}'''.format(task_prefix, i), return_tensors='pt').input_ids\n",
    "#   outputs = model.generate(input_ids)\n",
    "    outputs = model.generate(input_ids, max_new_tokens=50)  # Adjust this value as needed\n",
    "\n",
    "    print(i, tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('my-tf-en-to-sp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "!tar -czf my_model.tar.gz my-tf-en-to-sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The warning you're encountering comes from the `tokenizers` library used by Hugging Face, and it appears when the process is forked after parallelism has been used. This can potentially lead to deadlocks, so the library disables parallelism as a precaution.\n",
    "\n",
    "### Solutions to Avoid the Warning\n",
    "\n",
    "1. **Set the Environment Variable**:\n",
    "   You can explicitly set the `TOKENIZERS_PARALLELISM` environment variable to either `true` or `false` depending on whether you want to enable or disable parallelism.\n",
    "\n",
    "   To disable parallelism and suppress the warning, run:\n",
    "   ```bash\n",
    "   export TOKENIZERS_PARALLELISM=false\n",
    "   ```\n",
    "\n",
    "   If you're running this command inside a Python script or Jupyter notebook, you can set the environment variable within the script:\n",
    "   ```python\n",
    "   import os\n",
    "   os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "   ```\n",
    "\n",
    "2. **Avoid Using `tokenizers` Before Forking**:\n",
    "   If possible, structure your code so that the `tokenizers` library is not used before the process forks. This might involve reordering code or separating certain tasks.\n",
    "\n",
    "3. **Ignore the Warning**:\n",
    "   If the warning doesn't impact your workflow, you can choose to ignore it. The library will handle the situation by disabling parallelism automatically.\n",
    "\n",
    "### Using `tar` to Compress the Model\n",
    "\n",
    "The warning is unrelated to the `tar` command you used to compress your model directory. The command:\n",
    "\n",
    "```bash\n",
    "tar -czf my_model.tar.gz my-tf-en-to-sp\n",
    "```\n",
    "\n",
    "is correct and will create a `my_model.tar.gz` file containing the contents of the `my-tf-en-to-sp` directory. If the warning appears right after this command, it's likely due to some part of the code or environment that was executed before or after this command involving the `tokenizers` library."
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
